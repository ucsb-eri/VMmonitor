#!/usr/bin/env python
# This is pretty self contained at this point, but when we start doing the 
# data displays, there is some external information that becomes really useful
# sponsor, IP, etc...  Next step is to figure out if we want to add that info
# into the db during collection phase or require attachment at the display phase.
# a little torn on that at this point.
# get structure of the table
# select sql from sqlite_master where type='table';
# sqlite3 dbfile '.schema oldtable' | sed '1s/oldtable/newtable/' | sqlite3 dbfile
# INSERT INTO Destination SELECT * FROM Source;

import sys

# just bail if the python version is too old (ie: 2.4.x circa CentOS-5.x)
# this to quiet errors from older CentOS-5.x systems
if sys.version_info[0] == 2 and sys.version_info[1] < 6 :
    #if opt.debug:
    #    print "python version is not high enough, bailing"
    sys.exit(0)

import os

import glob
import tempfile
import time
from optparse import OptionParser
import subprocess


# these imports are not available in python-2.4.x
import ctypes
import io
import json
import sqlite3
import xml.etree.ElementTree as ET

p = OptionParser()
p.add_option('--prefix',
             action='store', dest='prefix', default='VMStatus',
             help='Prefix for generated JSON files (default: VMStatus)')
p.add_option('--JSONpath',
             action='store', dest='JSONpath', default='./',
             help='Path to the directory where JSON files live (default: ./)')
p.add_option('--output_file',
             action='store_true', dest='toFile', default=False,
             help='Generate a .json file in JSONpath instead of stdout')
p.add_option('--DBpath','--dbpath',
             action='store', dest='DBpath', default='./virtu.sqlite3',
             help='Path to the database (default: ./virtu.sqlite3)')
p.add_option('--nsdb','--nsdbpath',
             action='store', dest='nsdbpath', default='../www/generated-dbs/nsmgmt.sqlite3',
             help='Path to the name services database (default: ../www/generated-dbs/nsmgmt.sqlite3)')
p.add_option('--verbose', '-v',
             action='store_true', dest='verbose', default=False,
             help='increases verbosity of messages to stdout')
p.add_option('--debug', '-d',
             action='store_true', dest='debug', default=False,
             help='adds debugging output')
p.add_option('--collector_mode', '-c',
             action='store_false', dest='reporter', default=True,
             help='The mode in which the script collects data generated by reporters, '
             'and writes data into the db specified by DBpath. '
             'If the DB does not exist, this script will create a one. ')
p.add_option('-p', '--post',
             dest='post', 
             #default='http://thorin.eri.ucsb.edu:3000',
             default='',
             help='Outputs data to VMMonitor via HTTP-POST to the address specified.')

# Definition:
# https://libvirt.org/html/libvirt-libvirt-domain.html#virDomainState
StateDictionary = {
    0: 'NOSTATE',
    1: 'RUNNING',
    2: 'BLOCKED',
    3: 'PAUSED',
    4: 'SHUTDOWN',
    5: 'SHUTOFF',
    6: 'CRASHED',
    7: 'PMSUSPENDED',
    8: 'LAST'
}

class DomainData:
    ''' JSON serializable class for virtDomain'''
    def __init__(self, virtDomain):
        self.name = virtDomain.name()
        self.OSType = virtDomain.OSType()
        self.state = StateDictionary[virtDomain.info()[0]]
        self.maxMem = virtDomain.info()[1]  # KBytes
        self.memoryUsed = virtDomain.info()[2]  # KBytes
        self.numOfCpu = virtDomain.info()[3]
        self.cpuTime = virtDomain.info()[4]  # nanosecond

        root = ET.fromstring(virtDomain.XMLDesc(0))  # Parse XML
        # get .img path from XML
        self.path = root.find('devices/disk/source').get('file')
        self.FSType = getFSType(self.path)

# want to add additional info to this more easily...  Maybe an internal
# dict??
class OutputData:
    '''JSON serializable class for final output'''
    def __init__(self, hostName, activeDomains, inactiveDomains):
        #self.hostName = hostName
        self.activeVMs = activeDomains
        self.inactiveVMs = inactiveDomains
        #self.generateTime = int(time.time())
        self.hostData = {}
    def addHostData(self,key,value):
        self.hostData[key] = value

def handler(ctxt, err):
    global errno
    errno = err

def getFSType(path):
    # check_output is new in python 2.7, not all of our systems have that, so we need
    # to redo this using subprocess itself
    # if the guest VM image exists, run a linux stat on it to determine the filesystem type
    if os.path.exists(path):
        output = subprocess.Popen(['stat', '-f', '-L', '-c', '%T', path], stdout=subprocess.PIPE).communicate()[0]
        return output.rstrip()
    else:
        return "N/A"
    #return check_output(['stat', '-f', '-L', '-c', '%T', path]).rstrip()

def reporterWork():
    if opt.debug:
        print "reporterWork():"
    # run some additional checks to see if we are likely running 
    # if /usr/bin/virsh exists
    # if /etc/libvirt exists
    # if /var/lib/libvirt 
    # see if libvirtd is running
    # If all the above are met, then we can try to import libvirt
    
    # quick check to verify that libvirt is actually running
    # this to quiet errors from systems that exited with 1 because libvirt is 
    # enabled, but not actually running
    # But this is only for the reporting side, collection side doesn't care
    if not os.path.exists('/var/run/libvirt/libvirt-sock'):
        if opt.debug:
            print "libvirt-sock doesn't exist"
        sys.exit(0)

    #print "getting into doReporterWork"
    # this is actually conditional on the mode we are in
    try:
        import libvirt
    except ImportError:
        if opt.debug:
            print "failed to import libvirt"
        sys.exit(0)
        
    try:
        import multiprocessing
        cpucount = multiprocessing.cpu_count()
    except (ImportError, NotImplementedError):
        cpucount = 1
        pass
    if opt.post:
        try:
            import urllib2
        except ImportError:
            if opt.debug:
                print "import of urllib2 failed"
            sys.exit(1)

    mem_bytes = os.sysconf('SC_PAGE_SIZE') * os.sysconf('SC_PHYS_PAGES')  # e.g. 4015976448
    mem_gib = mem_bytes/(1024.**3)  # e.g. 3.74

    if opt.debug:
        print "memory in bytes:",mem_bytes

    # Register an error handler. Otherwise it will print a stderr.
    libvirt.registerErrorHandler(handler, 'Creating a connection')
    # Open connection to the hypervisor
    conn = libvirt.openReadOnly(None)

    if conn == None:
        if opt.verbose or opt.debug:
            print 'Failed to open connection to the hypervisor'
        sys.exit(2)

    hypervisor_name = conn.getHostname()

    if opt.debug:
        print "found hypervisor name:",hypervisor_name
    # Process active domains
    activeVirtDomainIDs = conn.listDomainsID()
    activeDomains = []
    for id in activeVirtDomainIDs:
        try:
            virtDomain = conn.lookupByID(id)
        except:
            if opt.verbose or opt.debug:
                print 'Failed to find domain with ID: ' + id
            sys.exit(3)
            
        domainData = DomainData(virtDomain)
        activeDomains.append(domainData.__dict__)

    # Process inactive domains
    inactiveVirtDomainNames = conn.listDefinedDomains()
    inactiveDomains = []
    for name in inactiveVirtDomainNames:
        try:
            virtDomain = conn.lookupByName(name)
        except:
            if opt.verbose:
                print 'Failed to find domain with name: ' + name
            sys.exit(5)
            
        domainData = DomainData(virtDomain)
        inactiveDomains.append(domainData.__dict__)

    if opt.debug:
        print "done with domain processing"
        
    # Generate output
    outputData = OutputData(hypervisor_name, activeDomains, inactiveDomains)
    outputData.addHostData('cpuCount',cpucount)
    outputData.addHostData('fqdn',hypervisor_name)
    outputData.addHostData('host',hypervisor_name.split('.')[0])
    outputData.addHostData('memoryBytes',mem_bytes)
    outputData.addHostData('memoryGBytes',mem_gib)
    outputData.addHostData('generateTime',int(time.time()))
    if opt.toFile:
        f = open(opt.JSONpath + opt.prefix +
        str(outputData.generateTime) + '.json', 'w')
        f.write(json.dumps(outputData.__dict__, sort_keys=True, indent=2))
    elif opt.post:
        if opt.debug:
            print "getting ready to post data to:",opt.post
        data = json.dumps(outputData.__dict__, sort_keys=True)
        host = 'thorin'
        try:
            port = os.environ['PORT']
        except KeyError:
            port = 3000

        url = 'http://%s:%d' % (host, port)
        url = opt.post
        try:
            req = urllib2.Request(url)
            req.add_header('Content-Type', 'application/json')
            urllib2.urlopen(req, data)
        except urllib2.URLError, e:
            print "urllib2 exception:", e
            sys.exit(6)
            
        if opt.debug:
            print "done pushing post data to:",opt.post
    else:
        print json.dumps(outputData.__dict__, sort_keys=True)

# want to condense/compact the data every so often (maybe even as a running entry)
# want to be able to pull the host history for each guest (ie: on hostA date to date, hostB date to date)


# this is called once for each json file (VM host)
# Would be nice if db structure could easily handle joining on the most recent guest entry
# for each host
# so maybe one table for hosts, one for guests, maybe a table for latestGuest
# maybe a history table with begin and end datestamps for each guest
# guest, host, begds, endds
# Have to figure out if we want to use foreign keys or just names, foreign keys may not make too much sense, since things 
# can change over the lifetime of a guest.  We would still have to keep config changes.  Do we want a guest history entry 
# to change with a change in params?  If so we would want to do some sort of hash/checksum field

# Also, just added some data fields to the hostData above, so need to rework the db stuff.
# This needs to more flexibly handle newer/older data (filling in defaults to certain db 
# fields as needed (or manually filling later))


# tables:
# 1. host - most current entry, or add in new entry if memory or cpu changes
# 2. guests - create a new record every time a new json file is processed
# 3. latest - only one entry per guest. update when host changes
# 4. history? - 

def writeRecords(c, path):
    f = open(path, 'r')
        
    try:
        inputJSON = json.load(f)
    except ValueError as err:
        if opt.debug:
            print(err)
        return 1
    
    host_schema = 'host TEXT unique, ds TEXT, cpu INTEGER, fqdn TEXT, ctime INTEGER, mem INTEGER'
    g_schema='guest TEXT, host TEXT, ds TEXT, ctime INTEGER, osType TEXT, fsType TEXT, path TEXT, cpuTime INTEGER, memMax INTEGER default 0, memUsed INTEGER default 0, cpuMax INTEGER default 1, cpuUsed INTEGER default 1, state TEXT'
    latest_schema=g_schema+', UNIQUE(guest, host) ON CONFLICT REPLACE'
    guest_schema=g_schema
    #history_schema = 'guest TEXT, host TEXT, dateBeg TEXT, dateEnd TEXT, UNIQUE(guest, host)'

    #   with generateTime (ctime) we can create current datestamp (YYYY-MM-DD)
    #   ds TEXT is datestamp, look at date/time functions from generateTime

    # Create a table for each Host if not exists

    c.execute('CREATE TABLE IF NOT EXISTS hosts (' + host_schema + ')')
    c.execute('CREATE TABLE IF NOT EXISTS guests (' + guest_schema + ')')
    c.execute('CREATE TABLE IF NOT EXISTS latest (' + latest_schema + ')')
    # c.execute('CREATE TABLE IF NOT EXISTS history (' + history_schema + ')')

    # Things we want:
    #   current guest/host mapping both directions
    #   history to be able to pull up historic mappings
    #   history will be for unique combinations of host+guest keys for adjacent spans of days
    #   Need to figure out how to sort/filter those adjacent entries and join them together
    #   Occasionally clean up old guest/host entries
    # dont need history on the host
    # can get current list of hosts from latest
    # I want a datestamp field as well as ctime
    # That can be done via sqlite during the insert
    json2hostMap={
        'host'         : 'host',
        #datestamp
        'cpuCount'     : 'cpu',
        'fqdn'         : 'fqdn',
        'generateTime' : 'ctime',
        'memoryBytes'  : 'mem',
    }
    json2guestMap={ 
        'name'         : 'guest',
        #host (eg. zippy),
        #datestamp - translate hostdata generateTime to YYYY-MM-DD,
        #ctime - from hostdata generateTime,
        'OSType'       : 'osType',
        'FSType'       : 'fsType',
        'path'         : 'path',
        'maxMem'       : 'memMax',
        'memoryUsed'   : 'memUsed',
        'numOfCpu'     : 'cpuUsed',
        'numOfCpu'     : 'cpuMax',
        'state'        : 'state',
    }

    if 'hostData' not in inputJSON.keys():
        print "JSON data doesn't conform to the expected format"
        return 2
        
    hostdata = inputJSON['hostData']

    # forming data, output host data to db
    from datetime import datetime as dt
    data = (
        hostdata['host'],
        dt.fromtimestamp(hostdata['generateTime']).strftime('%Y-%m-%d'),
        hostdata['cpuCount'],
        hostdata['fqdn'],
        hostdata['generateTime'],
        hostdata['memoryBytes'],
    )
    c.execute('INSERT OR REPLACE INTO hosts '
        'VALUES (?, ?, ?, ?, ?, ?)', data)

    guests = []
    #writing guests into db
    for VM in inputJSON['inactiveVMs']:
        guests.append(VM)
    for VM in inputJSON['activeVMs']:
        guests.append(VM)
        
    for VM in guests:
        data = (
            VM['name'],                 #guest vm name
            hostdata['host'],           #host name
            dt.fromtimestamp(hostdata['generateTime']).strftime('%Y-%m-%d'),#datestamp
            hostdata['generateTime'],   #ctime,
            VM['OSType'],               #osType
            VM['FSType'],               #fsType
            VM['path'],                 #path
            VM['cpuTime'],              #cpuTime
            VM['maxMem'],               #memMax
            VM['memoryUsed'],           #memUsed
            VM['numOfCpu'],             #cpuMax
            VM['numOfCpu'],             #cpuUsed
            VM['state'],                #state
        )

        #writing guests
        c.execute('INSERT INTO guests '
                'VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)', data)
        c.execute('INSERT or REPLACE INTO latest '
                'VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)', data)
        #writing latest
            #   1. if record does not exist, write
        #c.execute('SELECT COUNT(1) FROM latest WHERE guest=?', (data[0],))
        #if c.fetchone()[0] == 0:
        #    c.execute('INSERT INTO latest '
        #        'VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)', data)
        #
        ##   2. if date is newer, update
        #c.execute('SELECT ds FROM latest where guest=?', (data[0],))
        #latestDay = c.fetchone()[0].split('-')[2]
        #if (latestDay < data[2].split('-')[2]):
        #    c.execute('REPLACE INTO latest '
        #        'VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)', data)
        #
        ##   3. if date is same then compare states. update if running
        #elif (latestDay == data[2].split('-')[2]):
        #    #check if it is running
        #    if data[12] == 'RUNNING':
        #        c.execute('REPLACE INTO latest '
        #            'VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)', data)

    return 0
    
# def doCollectorWork(options):
#     conn = sqlite3.connect(opt.DBpath)
#     c = conn.cursor()

#     # want to do this a bit differently than walk
#     # want to glob *.json files 
#     # rename them into a processed directory once done (create directory if needed)
#     # chdir(dir) 
#     # glob.glob('*.json')
#     for root, dirs, files in os.walk(opt.JSONpath):
#         for fileName in files:
#             if opt.prefix in fileName and 'IMPORTED_' not in fileName:
#                 path = os.path.join(root, fileName)
#                 writeRecords(c, path)
#                 os.rename(path, os.path.join(root, 'IMPORTED_' + fileName))

#     # Save (commit) the changes
#     conn.commit()
#     conn.close()

# dont like the old setup to rename with an IMPORTED_ prefix....  Lets just move the
# file to a processed directory...  Simpler and can be more easily reprocessed
def collectorWork():
    # process preexisting JSON files at /home/sysadm/reporting/dsdata-virtu to sqlite3 db
    if opt.debug:
        print "collectorWork()"
    conn = sqlite3.connect(opt.DBpath)
    c = conn.cursor()

    if os.path.isdir(opt.JSONpath):
        # without sorting we get files going into the db out of order, so latest may not be
        files = sorted(glob.glob(opt.JSONpath+'/*.json'))
        if opt.debug:
            print files

    archivedir = opt.JSONpath+'archived'
    if not os.path.isdir(archivedir):
        os.mkdir(archivedir)
    if not os.path.isdir(archivedir):
        print "unable to create directory: "+archivedir
        return

    for fileName in files:
        bn=os.path.basename(fileName)
        print "processing:",fileName
        if writeRecords(c, fileName) == 0:
            archivedf=archivedir+'/'+bn
            try:
                os.rename(fileName,archivedf)
            except OSError:
                print OSError
            
    # Save (commit) the changes
    conn.commit()
    conn.close()

def main():
    #print "getting past parse_args"
    if not opt.verbose:
        devnull = open(os.devnull, 'w')
        sys.stderr = devnull

    if opt.reporter:
        if opt.debug:
            print "reporting mode:"
        reporterWork()
    else:
        if opt.debug:
            print "collection mode:"
        collectorWork()


if __name__ == '__main__':
    (opt, args) = p.parse_args()
    main()
    if opt.debug:
        print "done"
    sys.exit(0)
    
    
